{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel Density Estimation with Python (BigDIPA 2016)\n",
    "\n",
    "**A tutorial using Python and scientific libraries to run kernel density estimation (KDE) of simulated images on a personal computer.**\n",
    "\n",
    "By Jihyun Park (`jihyunp@ics.uci.edu`)<br/>\n",
    "Department of Computer Science, University of California, Irvine\n",
    "\n",
    "October 27, 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "-------------------------\n",
    "Kernel density estimation (KDE) is a non-parametric way to estimate the probability density function.\n",
    "It takes a finite sample of data and make inferences about the underlying probability density function everywhere.\n",
    "\n",
    "The lab consists of two parts:\n",
    "### Part 1\n",
    "- Simulate 2-D data\n",
    "- Estimate probability density using KDE (`KernelDensity` package from scikit-learn)\n",
    "- Bandwidth Selection (Manual, Automated)\n",
    "- Contour plot, scatter plot\n",
    "\n",
    "### Part 2\n",
    "- Repeat the part 1 with different simulated data (Exercise for you!)\n",
    "- Use Bayes theorem to calculate and plot posterior probabilities\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "--------------------------------\n",
    "- Basic knowledge of probability.\n",
    "- Familiarity with programming and nD array computing (e.g. working with matrices in Matlab or numpy in python).\n",
    "- Python 3.5 or Python 2.7 with libraries : Jupyter (for ipython notebook), numpy, scipy, scikit-learn, matplotlib.\n",
    "- It is recommended to have the newest version of libraries installed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "----------------------------------\n",
    "- [Scikit-learn Density Estimation](http://scikit-learn.org/stable/modules/density.html) : Description and examples on density estimation including KDE.\n",
    "- [Scikit-learn KDE Package Documentation](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KernelDensity.html) : `KernelDensity` class documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before We Start : Import Packages\n",
    "-----------------------------\n",
    "\n",
    "You can import the packages anytime before using the package, but we are importing them before we start to make sure you have all the packages that we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import binomial, multivariate_normal, uniform, seed\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.linalg import sqrtm, det, inv\n",
    "\n",
    "from sklearn.datasets import fetch_species_distributions\n",
    "from sklearn.datasets.species_distributions import construct_grids\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.grid_search import GridSearchCV # if you have older version of sklearn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get an **`ImportError`** for **`model_selection`**, \n",
    "update your **`scikit-learn`** using one of these commands below in terminal window.\n",
    "\n",
    "- `$conda install scikit-learn` : If you installed python through anaconda\n",
    "- `$pip install -U scikit-learn`\n",
    "\n",
    "Or, you can try importing **`GridSearchCV`** from **`sklearn.grid_search`** package.\n",
    "\n",
    "- `from sklearn.grid_search import GridSearchCV`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1\n",
    "--------------------\n",
    "## 1. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Simulate data\n",
    "\n",
    "Randomly generate data from 5 Gaussians. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 50 X 50 image with 5 Gaussians\n",
    "nX = 50\n",
    "nY = nX\n",
    "n_gaussians = 5\n",
    "\n",
    "# Generate the same data by setting the seed\n",
    "seed(135)\n",
    "\n",
    "# Initialize\n",
    "Xdata = np.zeros((0,2))\n",
    "true_means = np.zeros((0,2))\n",
    "true_covs = []\n",
    "true_Ns = []\n",
    "\n",
    "for i in range(n_gaussians):\n",
    "    mean = uniform(5, nX-5, size=2)\n",
    "    var = uniform(3, 80)\n",
    "    cov_mat = np.array([[var, 0],[0, var]])\n",
    "    n_sample = int(round(uniform(30,60)))\n",
    "    # Generate data points\n",
    "    d = multivariate_normal(mean, cov_mat, size=n_sample)\n",
    "    # Update the variables\n",
    "    Xdata = np.concatenate((Xdata, d), axis=0)\n",
    "    true_means = np.concatenate((true_means, mean[np.newaxis,:]), axis=0)\n",
    "    true_covs.append(cov_mat)\n",
    "    true_Ns.append(n_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Plot generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot true mean and var\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "for i in range(n_gaussians):\n",
    "    # std as circle\n",
    "    circle = plt.Circle(true_means[i,:], np.sqrt(true_covs[i][0,0]), alpha=0.1, linewidth=0)\n",
    "    ax.add_artist(circle)\n",
    "    # means as dot\n",
    "    ax.scatter(true_means[i,0], true_means[i,1], s=30, linewidth=0)\n",
    "    ax.set_xlim(0,nX)\n",
    "    ax.set_ylim(0,nY)\n",
    "\n",
    "# Plot the data points\n",
    "ax.scatter(Xdata[:,0], Xdata[:,1], s=10, color='black', alpha=0.7, linewidth=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid arrays\n",
    "Before we plot the density, we will generate arrays for grids. This is because we want a density value for each grid location. We're going to generate 100 X 100 mesh grid. If you want to make the grid denser, change the number for **`ngrid`** in the below code to something larger.\n",
    "\n",
    "After generating the mesh grid, we're going to flatten the matrix to have (N x 2) shape so that it can be used as an input for other functions. <br\\> \n",
    "The variables **`X, Y, xy, `**and **`ngrid`**  will be used throughout the lab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate Mesh Grid for Plotting (ngrid x ngrid matrix)\n",
    "ngrid = 100\n",
    "xgrid = np.linspace(-25, 75, ngrid)\n",
    "ygrid = np.linspace(-25, 75, ngrid)\n",
    "\n",
    "X, Y = np.meshgrid(xgrid, ygrid) # Now we have (ngrid x ngrid) matrix\n",
    "\n",
    "# ravel() function flattens (ngrid x ngrid) matrix -> (1 x ngrid**2) array\n",
    "xy = np.vstack([Y.ravel(), X.ravel()]).T\n",
    "xy.shape  # Print the shape!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Plot the true probability density for the simulated data\n",
    "This is the true (ideal) density plotted as contour plot. Using KDE with simulated data points, we want estimated density close to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Mixture weights are inferred from the number of data points.\n",
    "total_N = float(sum(true_Ns))\n",
    "true_prob = np.zeros((xy.shape[0]))\n",
    "for m, c, n in zip(true_means, true_covs, true_Ns):\n",
    "    true_prob += stats.multivariate_normal.pdf(xy, m, c) * n / total_N\n",
    "true_prob = true_prob.reshape((ngrid, ngrid), order='F')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "ax.contourf(X, Y, true_prob,\n",
    "            levels=np.linspace(true_prob.min(), true_prob.max(), 20), cmap='Reds')\n",
    "ax.scatter(true_means[:,0], true_means[:,1], s=35, c='black', label=\"True Means\")\n",
    "\n",
    "ax.legend(loc='lower right', scatterpoints=1)\n",
    "ax.set_title('True Density')\n",
    "ax.set_xlim(0,nX)\n",
    "ax.set_ylim(0,nY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define a wrapper function that runs KDE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define a wrapper function that runs KDE and returns the evaluated density in `(ngrid X ngrid)` matrix form. <br/>\n",
    "It is better to define a wrapper function since this will be used a lot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_kde(Xdata, bandwidth, metric, kernel):\n",
    "    \"\"\" Construct a KernelDensity object, fit with the data points we generated, \n",
    "        and then return the evaluated density for the (ngrid X ngrid) mesh grid \"\"\"\n",
    "    # Construct a kernel density object\n",
    "    kde = KernelDensity(bandwidth=bandwidth, metric=metric, kernel=kernel)\n",
    "    kde.fit(Xdata)\n",
    "    # kde.score_samples() returns values in log scale\n",
    "    phat = np.exp(kde.score_samples(xy))\n",
    "    phat = phat.reshape((ngrid, ngrid), order='F')\n",
    "    return phat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Plot the estimated density with true means\n",
    "\n",
    "Now run the function defined above, and plot the estimated density $\\hat{p}(x)$ as contour plot. \n",
    "We're going to use Euclidean distance (`metric='euclidean`) and Gaussian kernel (`kernel='gaussian'`), and change the size of the bandwidth to see the difference in the result. <br/>\n",
    "(You can also try with different metrics or kernels : more information at [Scikit-learn KernelDensity](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KernelDensity.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Small bandwidth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get estimated density from KDE\n",
    "phat = run_kde(Xdata, bandwidth=1, metric='euclidean', kernel='gaussian')\n",
    "\n",
    "fig, axs = plt.subplots(1,2,figsize=(11,5), sharex=True, sharey=True)\n",
    "\n",
    "# plot estimated probability density from KDE\n",
    "ax = axs[0]\n",
    "levels = np.linspace(phat.min(), phat.max(), 20)\n",
    "im = ax.contourf(X, Y, phat, levels=levels, cmap='Reds')\n",
    "ax.scatter(true_means[:,0], true_means[:,1], s=35, c='black', label=\"True Means\")\n",
    "ax.legend(loc='lower right', scatterpoints=1)\n",
    "ax.set_title('Density from KDE (Small Bandwidth, BW=1)')\n",
    "\n",
    "# Plot for true probability density\n",
    "ax = axs[1]\n",
    "ax.contourf(X, Y, true_prob,\n",
    "            levels=np.linspace(true_prob.min(), true_prob.max(), 20), cmap='Reds')\n",
    "ax.scatter(true_means[:,0], true_means[:,1], s=35, c='black', label=\"True Means\")\n",
    "ax.legend(loc='lower right', scatterpoints=1)\n",
    "ax.set_title('True Density')\n",
    "ax.set_xlim(0,nX)\n",
    "ax.set_ylim(0,nY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Large bandwidth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get estimated density from KDE\n",
    "phat = run_kde(Xdata, bandwidth=20, metric='euclidean', kernel='gaussian')\n",
    "\n",
    "fig, axs = plt.subplots(1,2,figsize=(11,5), sharex=True, sharey=True)\n",
    "\n",
    "# plot estimated probability density from KDE\n",
    "ax = axs[0]\n",
    "levels = np.linspace(phat.min(), phat.max(), 20)\n",
    "im = ax.contourf(X, Y, phat, levels=levels, cmap='Reds')\n",
    "ax.scatter(true_means[:,0], true_means[:,1], s=35, c='black', label=\"True Means\")\n",
    "ax.legend(loc='lower right', scatterpoints=1)\n",
    "ax.set_title('Density from KDE (Large Bandwidth, BW=20)')\n",
    "\n",
    "\n",
    "# Plot for true probability density\n",
    "ax = axs[1]\n",
    "ax.contourf(X, Y, true_prob,\n",
    "            levels=np.linspace(true_prob.min(), true_prob.max(), 20), cmap='Reds')\n",
    "ax.scatter(true_means[:,0], true_means[:,1], s=35, c='black', label=\"True Means\")\n",
    "ax.legend(loc='lower right', scatterpoints=1)\n",
    "ax.set_title('True Density')\n",
    "ax.set_xlim(0,nX)\n",
    "ax.set_ylim(0,nY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Manually Selected Bandwidth\n",
    "\n",
    "Try running with different size of bandwidths to find the best bandwidth. Modify the variable **`selected_bw`** in the below code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get estimated density from KDE\n",
    "selected_bw = \"\"\"Appropriate bandwidth size\"\"\"\n",
    "\n",
    "phat = run_kde(Xdata, bandwidth=selected_bw, metric='euclidean', kernel='gaussian')\n",
    "\n",
    "fig, axs = plt.subplots(1,2,figsize=(11,5), sharex=True, sharey=True)\n",
    "\n",
    "# plot estimated probability density from KDE\n",
    "ax = axs[0]\n",
    "levels = np.linspace(phat.min(), phat.max(), 20)\n",
    "im = ax.contourf(X, Y, phat, levels=levels, cmap='Reds')\n",
    "ax.scatter(true_means[:,0], true_means[:,1], s=35, c='black', label=\"True Means\")\n",
    "ax.legend(loc='lower right', scatterpoints=1)\n",
    "ax.set_title('Density from KDE (BW=%.2f)' % selected_bw)\n",
    "\n",
    "\n",
    "# Plot for true probability density\n",
    "ax = axs[1]\n",
    "ax.contourf(X, Y, true_prob,\n",
    "            levels=np.linspace(true_prob.min(), true_prob.max(), 20), cmap='Reds')\n",
    "ax.scatter(true_means[:,0], true_means[:,1], s=35, c='black', label=\"True Means\")\n",
    "ax.legend(loc='lower right', scatterpoints=1)\n",
    "ax.set_title('True Density')\n",
    "ax.set_xlim(0,nX)\n",
    "ax.set_ylim(0,nY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Automated Bandwidth Selection\n",
    "\n",
    "Cross-validation can be used to select the bandwidth automatically. Cross-validation is a model validation technique for assessing how the results will generalize to an independent data set. In K-fold cross-validation, randomized data are splitted into K sets, and K-1 sets are used for estimating the density (train set) and 1 set is used for evaluation (validation set). We do this for K times, and score is calculated for the validation set at each run. The overall cross-validation score is the mean of the M scores. (More info: [Wikipedia: Cross-validation](https://en.wikipedia.org/wiki/Cross-validation_%28statistics%29))\n",
    "\n",
    "Scikit-learn has a nice package called **`GridSearchCV`** that does all the job for us! It uses **`score()`** function in the object to calculate the score. **`KernelDensity`** class has a function **`score(valX)`** that returns the total log probability of the validation data **`valX`** under the model. **`GridSearchCV`** will calculate the cross-validation score for each bandwidth value, and then return the bandwidth that gave the highest score.\n",
    "\n",
    "We will use 10-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grid = GridSearchCV(KernelDensity(metric='euclidean', kernel='gaussian'),\n",
    "                    {'bandwidth': np.linspace(0.5, 10, 50)}, cv=10) # 10-fold cross-validation\n",
    "grid.fit(Xdata)\n",
    "print(grid.best_params_)\n",
    "bw_cv = grid.best_params_['bandwidth'] # Bandwidth value saved in 'bw_cv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get estimated density from KDE\n",
    "phat = run_kde(Xdata, bandwidth=bw_cv, metric='euclidean', kernel='gaussian')\n",
    "\n",
    "fig, axs = plt.subplots(1,2,figsize=(11,5), sharex=True, sharey=True)\n",
    "\n",
    "# plot estimated probability density from KDE\n",
    "ax = axs[0]\n",
    "levels = np.linspace(phat.min(), phat.max(), 20)\n",
    "im = ax.contourf(X, Y, phat, levels=levels, cmap='Reds')\n",
    "ax.scatter(true_means[:,0], true_means[:,1], s=35, c='black', label=\"True Means\")\n",
    "ax.legend(loc='lower right', scatterpoints=1)\n",
    "ax.set_title('Density from KDE (BW=%.2f)' % bw_cv)\n",
    "\n",
    "\n",
    "# Plot for true probability density\n",
    "ax = axs[1]\n",
    "ax.contourf(X, Y, true_prob,\n",
    "            levels=np.linspace(true_prob.min(), true_prob.max(), 20), cmap='Reds')\n",
    "ax.scatter(true_means[:,0], true_means[:,1], s=35, c='black', label=\"True Means\")\n",
    "\n",
    "ax.legend(loc='lower right', scatterpoints=1)\n",
    "ax.set_title('True Density')\n",
    "ax.set_xlim(0,nX)\n",
    "ax.set_ylim(0,nY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# PART 2 \n",
    "--------------------\n",
    "In this part, we will generate two different data from two different Gaussian functions. \n",
    "\n",
    "\n",
    "### Drawing Ellipse\n",
    "We will define a function that returns a list of (x,y) pairs for an ellipse. This will be used for plotting the standard deviation of Gaussian function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper function (For Drawing ellipse)\n",
    "def get_ellipse(mean, cov_mat):\n",
    "    theta = np.linspace(0,2*np.pi,100)\n",
    "    circle = np.array([np.sin(theta), np.cos(theta)]).T\n",
    "    ell = np.dot(circle, sqrtm(cov_mat))\n",
    "    ell = ell + np.tile(mean, (ell.shape[0],1))\n",
    "    return ell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Gaussian 1 (Red)\n",
    "Generate data from a single 2-D Gaussian, and store the information in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed(1)\n",
    "m = np.array([35, 35])\n",
    "cov = np.array([[50,30],[30,50]])\n",
    "N = 120\n",
    "data = multivariate_normal(m, cov, size=N)\n",
    "Gaussian1 = {'mean':m, 'cov':cov, 'N':N, 'data':data}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the data points with its true mean and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "\n",
    "ax.scatter(Gaussian1['mean'][0], Gaussian1['mean'][1], s=30, color='black', linewidth=0)\n",
    "ell1 = get_ellipse(Gaussian1['mean'], Gaussian1['cov'])\n",
    "ax.plot(ell1[:,0], ell1[:,1], color='red')\n",
    "# Plot the data points\n",
    "ax.scatter(Gaussian1['data'][:,0], Gaussian1['data'][:,1], s=10, color='red', alpha=0.7, linewidth=0)\n",
    "\n",
    "ax.set_xlim(0,nX)\n",
    "ax.set_ylim(0,nY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE\n",
    "Run KDE and plot the estimated density as contour plot for **`Gaussian1`**. Select a proper bandwidth size using **automated bandwidth selection**.  Fill in the parts that are missing (parts that are commented out with three double quotes : `\"\"\"`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Use GridSearchCV to find the most appropriate bandwidth size (Search from 1 to 6)\"\"\"\n",
    "\n",
    "bw_red = \"\"\"Selected bandwidth size\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phat_red = run_kde(\"\"\"Fill in this part using 'Gaussian1' and 'bw_red'\"\"\")\n",
    "\n",
    "# plot contours of the density\n",
    "fig, ax = plt.subplots(figsize=(6,5))\n",
    "levels = np.linspace(phat_red.min(), phat_red.max(), 20)\n",
    "im = ax.contourf(X, Y, phat_red, levels=levels, cmap='Reds')\n",
    "fig.colorbar(im)\n",
    "\n",
    "ax.scatter(Gaussian1['mean'][0], Gaussian1['mean'][1], s=35, c='black', label=\"True Means\")\n",
    "ax.legend(loc='lower right', scatterpoints=1)\n",
    "ax.set_title('$p(x|C=1)$')\n",
    "ax.set_xlim(0,nX)\n",
    "ax.set_ylim(0,nY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gaussian 2 (Blue)\n",
    "Generate data from a single 2-D Gaussian, and store the information in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Gaussian 2\n",
    "seed(1)\n",
    "m = np.array([20, 20])\n",
    "cov = np.array([[55,20],[20,55]])\n",
    "N = 150\n",
    "data = multivariate_normal(m, cov, size=N)\n",
    "Gaussian2 = {'mean':m, 'cov':cov, 'N':N, 'data':data}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the data points with mean and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "ax.scatter(Gaussian2['mean'][0], Gaussian2['mean'][1], s=30, color='black', linewidth=0)\n",
    "ell2 = get_ellipse(Gaussian2['mean'], Gaussian2['cov'])\n",
    "ax.plot(ell2[:,0], ell2[:,1], color='blue')\n",
    "\n",
    "# Plot the data points\n",
    "ax.scatter(Gaussian2['data'][:,0], Gaussian2['data'][:,1], s=10, color='blue', alpha=0.7, linewidth=0)\n",
    "\n",
    "ax.set_xlim(0,nX)\n",
    "ax.set_ylim(0,nY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE\n",
    "Run KDE and plot the estimated density as contour plot for **`Gaussian2`**. Select a proper bandwidth size using **automated bandwidth selection**.  Fill in the parts that are missing (parts that are commented out with three double quotes : `\"\"\"`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Use GridSearchCV to find the most appropriate bandwidth size (Search from 1 to 6)\"\"\"\n",
    "\n",
    "bw_blue = \"\"\"Selected bandwidth size\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phat_blue =run_kde(\"\"\"Fill in this part using 'Gaussian2' and 'bw_blue'\"\"\")\n",
    "\n",
    "# plot contours of the density\n",
    "fig, ax = plt.subplots(figsize=(6,5))\n",
    "levels = np.linspace(phat_blue.min(), phat_blue.max(), 20)\n",
    "im = ax.contourf(X, Y, phat_blue, levels=levels, cmap='Blues')\n",
    "fig.colorbar(im)\n",
    "\n",
    "ax.scatter(Gaussian2['mean'][0], Gaussian2['mean'][1], s=35, c='black', label=\"True Means\")\n",
    "ax.legend(loc='lower right', scatterpoints=1)\n",
    "ax.set_xlim(0,nX)\n",
    "ax.set_ylim(0,nY)\n",
    "ax.set_title('$p(x|C=2)$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gaussian 1 + Gaussian 2\n",
    "Now plot both of the data from **`Gaussian1`** and **`Gaussian2`** in the same figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "\n",
    "# Gaussian 1\n",
    "ax.scatter(Gaussian1['mean'][0], Gaussian1['mean'][1], s=30, color='black', linewidth=0)\n",
    "ell = get_ellipse(Gaussian1['mean'], Gaussian1['cov'])\n",
    "ax.plot(ell[:,0], ell[:,1], color='red')\n",
    "# Plot the data points\n",
    "ax.scatter(Gaussian1['data'][:,0], Gaussian1['data'][:,1], s=10, color='red', alpha=0.7, linewidth=0)\n",
    "\n",
    "# Gaussian 2\n",
    "ax.scatter(Gaussian2['mean'][0], Gaussian2['mean'][1], s=30, color='black', linewidth=0)\n",
    "ell = get_ellipse(Gaussian2['mean'], Gaussian2['cov'])\n",
    "ax.plot(ell[:,0], ell[:,1], color='blue')\n",
    "# Plot the data points\n",
    "ax.scatter(Gaussian2['data'][:,0], Gaussian2['data'][:,1], s=10, color='blue', alpha=0.7, linewidth=0)\n",
    "    \n",
    "ax.set_xlim(0,nX)\n",
    "ax.set_ylim(0,nY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE\n",
    "Run KDE and plot the estimated density as contour plot for the data plotted above. Select a proper bandwidth size using **manual bandwidth selection**.<br/>\n",
    "For concatenating two different `numpy.ndarray` **`a`** and **`b`**, use **`np.concatenate((a,b))`**.   Fill in the parts that are missing (parts that are commented out with three double quotes : `\"\"\"`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_total = np.concatenate(\"\"\"Fill in this part\"\"\")\n",
    "phat_total = run_kde(\"\"\"Fill in this part using 'data_total\"\"\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "\n",
    "levels = np.linspace(phat_total.min(), phat_total.max(), 20)\n",
    "ax.contourf(X, Y, phat_total, levels=levels, cmap='Purples')\n",
    "ax.scatter(Gaussian1['mean'][0], Gaussian1['mean'][1], s=35, c='black', label=\"True Means\")\n",
    "ax.scatter(Gaussian2['mean'][0], Gaussian2['mean'][1], s=35, c='black')\n",
    "ax.legend(loc='lower right', scatterpoints=1)\n",
    "ax.set_xlim(0,nX)\n",
    "ax.set_ylim(0,nY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Posterior probability : Choosing the most likely class\n",
    "\n",
    "We use posterior probabilities to select a class that is more likely given an input. The **class** is either Gaussian 1 (Red) or Gaussian 2 (Blue). \n",
    "Posterior probabilities can be calculated using below equation. Note that the estimated densities in section 1 and 2 correspond to $\\hat{p}(x|Red)$ and $\\hat{p}(x|Blue)$, respectively. \n",
    "\n",
    "$p(Red|x) \\propto p(x|Red) \\times P(Red)$\n",
    "\n",
    "$p(Blue|x) \\propto p(x|Blue) \\times P(Blue)$\n",
    "\n",
    "\n",
    "If $p(Red|x) > p(Blue|x)$ for a given input $x$, it means that $x$ is more likely to be generated from Gaussian 1 (Red). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# P(Red), P(Blue) (Priors are inferred from the number of data points)\n",
    "N_red = Gaussian1['N']\n",
    "N_blue = Gaussian2['N']\n",
    "N_total = float(N_red + N_blue)\n",
    "p_red = N_red/N_total\n",
    "p_blue = N_blue/N_total\n",
    "\n",
    "# Calculate Posterior (Note P(X|Red) = phat_red, P(X|Blue) = phat_blue)\n",
    "p_red_x_ = phat_red * p_red\n",
    "p_blue_x_ = phat_blue * p_blue\n",
    "norm_factor = p_red_x_ + p_blue_x_\n",
    "\n",
    "# Normalize\n",
    "p_red_x = p_red_x_ / norm_factor\n",
    "p_blue_x = p_blue_x_ / norm_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True posterior probabilities are also calculated. **`scipy.stats.multivariate_normal.pdf()`** was used to generate the true 2-D normal densities. We're going to assume the true priors are the same as the estimated priors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# True posterior (Assume the same P(red), and P(blue))\n",
    "p_x_red = stats.multivariate_normal.pdf(xy, Gaussian1['mean'], Gaussian1['cov'])\n",
    "p_red_x_true_ = p_x_red * p_red\n",
    "\n",
    "p_x_blue = stats.multivariate_normal.pdf(xy, Gaussian2['mean'], Gaussian2['cov'])\n",
    "p_blue_x_true_ = p_x_blue * p_blue\n",
    "norm_factor = p_red_x_true_ + p_blue_x_true_\n",
    "\n",
    "# Normalize & reshape\n",
    "p_red_x_true = p_red_x_true_ / norm_factor\n",
    "p_blue_x_true = p_blue_x_true_ / norm_factor\n",
    "\n",
    "p_red_x_true = p_red_x_true.reshape((ngrid, ngrid), order='F')\n",
    "p_blue_x_true = p_blue_x_true.reshape((ngrid, ngrid), order='F')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot $p(Red|x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,5))\n",
    "levels = np.linspace(p_red_x.min(), p_red_x.max(), 20)\n",
    "im = ax.contourf(X, Y, p_red_x, levels=levels, cmap='Reds')\n",
    "fig.colorbar(im)\n",
    "ax.scatter(Gaussian1['mean'][0], Gaussian1['mean'][1], s=35, c='black', label=\"True Means\")\n",
    "ax.scatter(Gaussian2['mean'][0], Gaussian2['mean'][1], s=35, c='black')\n",
    "ax.legend(loc='lower right', scatterpoints=1)\n",
    "ax.set_xlim(0,nX)\n",
    "ax.set_ylim(0,nY)\n",
    "ax.set_title('$p(C=1|x)$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot $p(Blue|x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,5))\n",
    "levels = np.linspace(p_blue_x.min(), p_blue_x.max(), 20)\n",
    "im = ax.contourf(X, Y, p_blue_x, levels=levels, cmap='Blues')\n",
    "fig.colorbar(im)\n",
    "ax.scatter(Gaussian1['mean'][0], Gaussian1['mean'][1], s=35, c='black', label=\"True Means\")\n",
    "ax.scatter(Gaussian2['mean'][0], Gaussian2['mean'][1], s=35, c='black')\n",
    "ax.legend(loc='lower right', scatterpoints=1)\n",
    "ax.set_xlim(0,nX)\n",
    "ax.set_ylim(0,nY)\n",
    "ax.set_title('$p(C=2|x)$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the decision boundary\n",
    "\n",
    "Decision boundary is a boundary when $p(Red|x) = p(Blue|x)$, and since we only have two classes (Red, Blue), this is when $p(Red|x) = p(Blue|x) = 0.5$. \n",
    "\n",
    "The first figure is when we plot $p(Blue|x)$ with a different colormap. The white area is when $p(Blue|x) = 0.5$. The second figure has the estimated decision boundary and the true decision boundary overlaid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2, figsize=(11,5))\n",
    "\n",
    "# First subplot\n",
    "ax = axs[0]\n",
    "# Contour plot\n",
    "levels = np.linspace(p_blue_x.min(), p_blue_x.max(), 20)\n",
    "ax.contourf(X, Y, p_blue_x, levels=levels, cmap='RdBu')\n",
    "# Plot means\n",
    "ax.scatter(Gaussian1['mean'][0], Gaussian1['mean'][1], s=35, c='black', label=\"True Means\")\n",
    "ax.scatter(Gaussian2['mean'][0], Gaussian2['mean'][1], s=35, c='black')\n",
    "\n",
    "ax.legend(loc='lower right', scatterpoints=1)\n",
    "ax.set_xlim(0,nX)\n",
    "ax.set_ylim(0,nY)\n",
    "\n",
    "\n",
    "# Second subplot\n",
    "ax = axs[1]\n",
    "\n",
    "# Data points\n",
    "ax.scatter(Gaussian1['data'][:,0], Gaussian1['data'][:,1], linewidth=0, color='red', alpha=0.4)\n",
    "ax.scatter(Gaussian2['data'][:,0], Gaussian2['data'][:,1], linewidth=0, color='blue', alpha=0.4)\n",
    "# Contour plot\n",
    "ax.contour(X, Y, p_red_x- p_blue_x, levels=[0.0], \n",
    "           colors='black', linewidths=3)\n",
    "ax.contour(X, Y, p_red_x_true- p_blue_x_true, levels=[0.0], alpha=0.7,\n",
    "           colors='lime', linewidths=3, labels='True Decision Boundary')\n",
    "# Below lines are for the legend creation.\n",
    "ax.plot([],[], color='black', linewidth=3, label='Estimated Decision Boundary')\n",
    "ax.plot([],[], color='lime', linewidth=3, alpha=0.7, label='True Decision Boundary')\n",
    "ax.legend(loc='lower right', fontsize=11)\n",
    "ax.set_xlim(0,nX)\n",
    "ax.set_ylim(0,nY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### EXERCISE\n",
    "### Decision boundary when the bandwidth is too small or too large\n",
    "\n",
    "To see how the decision boundary changes with different size of bandwidth, let's re-calculate estimated density from KDE with different sizes of bandwidth, and plot the decision boundary. Fill in the parts that are missing (parts that are commented out with three double quotes : `\"\"\"`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "selected_bw = \"\"\"number for bandwidth\"\"\"\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(5,5))\n",
    "\n",
    "p_x_red = run_kde(\"\"\"Fill in this part\"\"\")\n",
    "p_x_blue = run_kde(\"\"\"Fill in this part\"\"\")\n",
    "\n",
    "# Posterior probability \n",
    "\"\"\"Feel free to add more lines of code\"\"\"\n",
    "p_red_x = \"\"\"Equation for posterior probability\"\"\"\n",
    "p_blue_x = \"\"\"Equation for posterior probability\"\"\"\n",
    "\n",
    "# Contour plot\n",
    "levels = np.linspace(p_blue_x.min(), p_blue_x.max(), 20)\n",
    "ax.contourf(X, Y, p_blue_x, levels=levels, cmap='RdBu', alpha=0.2)\n",
    "\n",
    "# Data points\n",
    "ax.scatter(Gaussian1['data'][:,0], Gaussian1['data'][:,1], linewidth=0, color='red', alpha=0.4)\n",
    "ax.scatter(Gaussian2['data'][:,0], Gaussian2['data'][:,1], linewidth=0, color='blue', alpha=0.4)\n",
    "\n",
    "# Estimated decision boundary\n",
    "ax.contour(X, Y, p_red_x - p_blue_x, levels=[0.0], \n",
    "           colors='black', linewidths=3)\n",
    "# True decision boundary\n",
    "ax.contour(X, Y, p_red_x_true - p_blue_x_true, levels=[0.0], alpha=0.6,\n",
    "           colors='lime', linewidths=3, labels='True Decision Boundary')\n",
    "\n",
    "# Below lines are for legend creation.\n",
    "ax.plot([],[], color='black', linewidth=3, label='Estimated Decision Boundary')\n",
    "ax.plot([],[], color='lime', linewidth=3, alpha=0.6, label='True Decision Boundary')\n",
    "ax.legend(loc='lower right', fontsize=11)\n",
    "ax.set_xlim(0,nX)\n",
    "ax.set_ylim(0,nY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you were able to fill in the previous part, you can use a for loop below to plot two different cases in one figure. We're going to have three subplots with small, medium, and large bandwidth. Fill in the parts that are missing (parts that are commented out with three double quotes `\"\"\"`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "small_bw = \"\"\"Integer value for small BW\"\"\"\n",
    "med_bw = 4\n",
    "large_bw = \"\"\"Integer value for large BW\"\"\"\n",
    "bandwidth_list = [small_bw, med_bw, large_bw]\n",
    "\n",
    "fig, axs = plt.subplots(1,3, figsize=(17,5))\n",
    "\n",
    "for bw, ax in zip(bandwidth_list, axs):\n",
    "\n",
    "    p_x_red = run_kde(\"\"\"Fill in this part\"\"\")\n",
    "    p_x_blue = run_kde(\"\"\"Fill in this part\"\"\")\n",
    "\n",
    "    # Posterior probability \n",
    "    \"\"\"Feel free to add more lines of code\"\"\"\n",
    "    p_red_x = \"\"\"Equation for posterior probability\"\"\"\n",
    "    p_blue_x = \"\"\"Equation for posterior probability\"\"\"\n",
    "    \n",
    "    # Contour plot\n",
    "    levels = np.linspace(p_blue_x.min(), p_blue_x.max(), 20)\n",
    "    ax.contourf(X, Y, p_blue_x, levels=levels, cmap='RdBu', alpha=0.2)\n",
    "    \n",
    "    # Data points\n",
    "    ax.scatter(Gaussian1['data'][:,0], Gaussian1['data'][:,1], linewidth=0, color='red', alpha=0.4)\n",
    "    ax.scatter(Gaussian2['data'][:,0], Gaussian2['data'][:,1], linewidth=0, color='blue', alpha=0.4)\n",
    "    \n",
    "    # Decision boundaries\n",
    "    ax.contour(X, Y, p_red_x- p_blue_x, levels=[0.0], \n",
    "               colors='black', linewidths=3)\n",
    "    ax.contour(X, Y, p_red_x_true- p_blue_x_true, levels=[0.0], alpha=0.6,\n",
    "               colors='lime', linewidths=3, labels='True Decision Boundary')\n",
    "    \n",
    "    ax.set_title('Estimated Decision Boundary (BW = %d)' % bw)\n",
    "\n",
    "    # Below lines are for legend creation.\n",
    "    ax.plot([],[], color='black', linewidth=3, label='Estimated Decision Boundary')\n",
    "    ax.plot([],[], color='lime', linewidth=3, alpha=0.6, label='True Decision Boundary')\n",
    "    ax.legend(loc='lower right', fontsize=11)\n",
    "    ax.set_xlim(0,nX)\n",
    "    ax.set_ylim(0,nY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE (OPTIONAL)\n",
    "### Decision boundary when the sample size is small or large\n",
    "\n",
    "Generate two sets of data from each Gaussian1 and Gaussian2, one with small sample size `N`, and one with large sample size `N`.\n",
    "\n",
    "Re-estimate the density from KDE with data of different sample sizes, and plot the decision boundaries. Fill in the parts that are missing (parts that are commented out with three double quotes : `\"\"\"`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate the data set with different sample sizes. \n",
    "\n",
    "# Gaussian 1\n",
    "seed(1)\n",
    "m = Gaussian1['mean']\n",
    "cov = Gaussian1['cov']\n",
    "\n",
    "N = Gaussian1['N'] / 5\n",
    "data = multivariate_normal(m, cov, size=N)\n",
    "Gaussian1_smallN = {'mean':m, 'cov':cov, 'N':N, 'data':data}\n",
    "\n",
    "N = Gaussian1['N'] * 20\n",
    "data = multivariate_normal(m, cov, size=N)\n",
    "Gaussian1_largeN = {'mean':m, 'cov':cov, 'N':N, 'data':data}\n",
    "\n",
    "\n",
    "# Gaussian 2\n",
    "m = Gaussian2['mean']\n",
    "cov = Gaussian2['cov']\n",
    "\n",
    "N = Gaussian1['N'] / 5\n",
    "data = multivariate_normal(m, cov, size=N)\n",
    "Gaussian2_smallN = {'mean':m, 'cov':cov, 'N':N, 'data':data}\n",
    "\n",
    "N = Gaussian1['N'] * 20\n",
    "data = multivariate_normal(m, cov, size=N)\n",
    "Gaussian2_largeN = {'mean':m, 'cov':cov, 'N':N, 'data':data}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to have 3 subplots: for small N, medium N, large N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_list = [[Gaussian1_smallN, Gaussian2_smallN], \n",
    "             [Gaussian1, Gaussian2], \n",
    "             [Gaussian1_largeN, Gaussian2_largeN]]\n",
    "\n",
    "fig, axs = plt.subplots(1,3, figsize=(17,5))\n",
    "\n",
    "for Gaussians, ax in zip(data_list, axs):\n",
    "\n",
    "    g1_data = Gaussians[0]['data']\n",
    "    g2_data = Gaussians[1]['data']\n",
    "    sample_size_N = Gaussians[0]['N'] + Gaussians[1]['N']\n",
    "    \n",
    "    p_x_red = run_kde(\"\"\"Fill in the parenthesis using variable g1_data\"\"\")\n",
    "    p_x_blue = run_kde(\"\"\"Fill in the parenthesis using variable g2_data \"\"\")\n",
    "\n",
    "    # Posterior \n",
    "    \"\"\"Feel free to add more lines of code\"\"\"\n",
    "    p_red_x = \"\"\"Equation for posterior probability\"\"\"\n",
    "    p_blue_x = \"\"\"Equation for posterior probability\"\"\"\n",
    "\n",
    "    # Contour plot\n",
    "    levels = np.linspace(p_blue_x.min(), p_blue_x.max(), 20)\n",
    "    ax.contourf(X, Y, p_blue_x, levels=levels, cmap='RdBu', alpha=0.2)\n",
    "    \n",
    "    # Data points\n",
    "    ax.scatter(Gaussians[0]['data'][:,0], Gaussians[0]['data'][:,1], linewidth=0, color='red', alpha=0.4)\n",
    "    ax.scatter(Gaussians[1]['data'][:,0], Gaussians[1]['data'][:,1], linewidth=0, color='blue', alpha=0.4)\n",
    "\n",
    "    # Decision boundaries\n",
    "    ax.contour(X, Y, p_red_x- p_blue_x, levels=[0.0], \n",
    "               colors='black', linewidths=3)\n",
    "    ax.contour(X, Y, p_red_x_true- p_blue_x_true, levels=[0.0], alpha=0.7,\n",
    "               colors='lime', linewidths=3, labels='True Decision Boundary')\n",
    "    \n",
    "    ax.set_title('Estimated Decision Boundary (N = %d)' % sample_size_N)\n",
    "\n",
    "    # Below lines are for the legend creation.\n",
    "    ax.plot([],[], color='black', linewidth=3, label='Estimated Decision Boundary')\n",
    "    ax.plot([],[], color='lime', linewidth=3, alpha=0.7, label='True Decision Boundary')\n",
    "    ax.legend(loc='lower right', fontsize=11)\n",
    "    \n",
    "    ax.set_xlim(0,nX)\n",
    "    ax.set_ylim(0,nY)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
