{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel Density Estimation for Mouse Cell Detection Data   \n",
    "\n",
    "**Using kernel density estimation (KDE) to analyze 2d cell detection data from flourescent imaging of stellate ganglia in mice.**\n",
    "\n",
    "By Jihyun Park (`jihyunp@ics.uci.edu`) and Padhraic Smyth (`smyth@ics.uci.edu`)<br/>\n",
    "Department of Computer Science, University of California, Irvine\n",
    "\n",
    "Presented as part of the UCI BigDIPA Lab\n",
    "\n",
    "September 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "-------------------------\n",
    "In this section of the Lab we will use the kernel density estimation functions to analyze data from cell detections from flourescent imaging of stellate ganglia (near the heart) from mice.\n",
    "\n",
    "This is unpublished data provided courtesy of Pradeep Rajendran and the UCLAÂ Cardiac Arrhythmia Center, under funding from NIH SPARC: Mapping the mammalian cardiac nervous system.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "--------------------------------\n",
    "- Familiarity with the main KDE python notebook for the BigDIPA course\n",
    "- Basic knowledge of probability.\n",
    "- Familiarity with programming and nD array computing (e.g. working with matrices in Matlab or numpy in python).\n",
    "- Python 3.5 with libraries : Jupyter (for ipython notebook), numpy, scipy, scikit-learn, matplotlib.\n",
    "- It is recommended to have the newest version of libraries installed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "----------------------------------\n",
    "- [Scikit-learn Density Estimation](http://scikit-learn.org/stable/modules/density.html) : Description and examples on density estimation including KDE.\n",
    "- [Scikit-learn KDE Package Documentation](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KernelDensity.html) : `KernelDensity` class documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages\n",
    "-----------------------------\n",
    "\n",
    "Import the necessary packages (you may have already imported them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.style.use('classic')\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.grid_search import GridSearchCV # if you have older version of sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get an **`ImportError`** for **`model_selection`**, \n",
    "you can try importing **`GridSearchCV`** from **`sklearn.grid_search`** package.\n",
    "\n",
    "- `from sklearn.grid_search import GridSearchCV`\n",
    "\n",
    "or, update your **`scikit-learn`** using one of these commands below in **terminal window**.\n",
    "\n",
    "- `conda install scikit-learn` : If you installed python through anaconda\n",
    "- `pip install -U scikit-learn`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "--------------------\n",
    "##  Mouse Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define the folder where data is located\n",
    "datapath = './spatial_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will import 3 files below, where each file contains information about cell detections for 3 different mice. Each file contains locations of cell detections obtained via flourescent imaging of stellate ganglia (near the heart) of individual mice.\n",
    "\n",
    "The format of each file is\n",
    "- one row per detected cell\n",
    "- first 3 columns are the x, y, z locations of the cells (here we will ignore the z dimension)\n",
    "- the 4th column is an integer indicating the region of interest (ROI) that the cell is located in. We will focus on ROI 2, i.e., cells that have a 2 in column 4 (this is the ROI where most of the cells are located for each mouse).\n",
    "\n",
    "The x,y locations of the cells for each mouse have been warped to try to register them, i.e., line up the data from each mouse as best as possible. This warping is not perfect, however, so you may see systematic shifts in the resulting data. The warping also means that the resulting x,y locations are not in any interpretable units\n",
    "\n",
    "Thanks to Charless Fowlkes for preprocessing the data (warping, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read in the mouse data\n",
    "filename = datapath + \"mouse1_SG_L_W_data.csv\"\n",
    "m_data1_all = pd.read_csv(filename).as_matrix()\n",
    "filename = datapath + \"mouse2_SG_L_W_data.csv\"\n",
    "m_data2_all = pd.read_csv(filename).as_matrix()\n",
    "filename = datapath + \"mouse5_SG_L_W_data.csv\"\n",
    "m_data5_all = pd.read_csv(filename).as_matrix() \n",
    "\n",
    "# Print the shape of the data. We have 4 columns\n",
    "print(m_data1_all.shape)\n",
    "print(m_data2_all.shape)\n",
    "print(m_data5_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m_data1_all[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take the second region (ROI=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r2_idx = np.where(m_data1_all[:,-1] == 2)[0]\n",
    "m_data1 = m_data1_all[r2_idx, :2]\n",
    "\n",
    "m_data1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Do the same thing for data2 and data5\n",
    "\n",
    "m_data2 = m_data2_all[np.where(m_data2_all[:,-1]==2)[0], :2]\n",
    "print(m_data2.shape)\n",
    "m_data5 = m_data5_all[np.where(m_data5_all[:,-1]==2)[0], :2]\n",
    "print(m_data5.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Scatter plot of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "# Plot the data points\n",
    "ax.scatter(m_data1[:,0], m_data1[:,1], s=10, color='blue', alpha=0.7, linewidth=0, label='Mouse1')\n",
    "ax.scatter(m_data2[:,0], m_data2[:,1], s=10, color='red', alpha=0.7, linewidth=0, label='Mouse2')\n",
    "ax.scatter(m_data5[:,0], m_data5[:,1], s=10, color='green', alpha=0.7, linewidth=0, label='Mouse5')\n",
    "ax.grid(alpha=0.3)\n",
    "ax.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Three different plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,3, figsize=(15,4), sharex=True, sharey=True)\n",
    "\n",
    "axs[0].scatter(m_data1[:,0], m_data1[:,1], s=10, color='blue', alpha=0.7, linewidth=0, label='Mouse1')\n",
    "axs[0].set_ylabel('Y')\n",
    "axs[0].set_xlabel('X')\n",
    "axs[0].set_title('Mouse 1')\n",
    "\n",
    "axs[1].scatter(m_data2[:,0], m_data2[:,1], s=10, color='red', alpha=0.7, linewidth=0, label='Mouse2')\n",
    "axs[1].set_xlabel('X')\n",
    "axs[1].set_title('Mouse 2')\n",
    "\n",
    "axs[2].scatter(m_data5[:,0], m_data5[:,1], s=10, color='green', alpha=0.7, linewidth=0, label='Mouse5')\n",
    "axs[2].set_xlabel('X')\n",
    "axs[2].set_title('Mouse 5')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid arrays\n",
    "Before we plot the density, we will generate arrays for grids. This is because we want a density value for each grid location. We're going to generate 100 X 100 mesh grid. If you want to make the grid denser, change the number for **`ngrid`** in the below code to something larger.\n",
    "\n",
    "After generating the mesh grid, we're going to flatten the matrix to have (N x 2) shape so that it can be used as an input for other functions. <br\\> \n",
    "The variables **`X, Y, xy, `**and **`ngrid`**  will be used throughout the lab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check the minimum and maximum values of one of the data\n",
    "print(np.min(m_data1[:,0]), np.max(m_data1[:,0]))\n",
    "print(np.min(m_data1[:,1]), np.max(m_data1[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate Mesh Grid for Plotting (ngrid x ngrid matrix)\n",
    "\n",
    "# limits and grids for the data\n",
    "lower_lim = 500\n",
    "upper_lim = 1500\n",
    "\n",
    "ngrid = 500\n",
    "\n",
    "xgrid = np.linspace(lower_lim, upper_lim, ngrid)\n",
    "ygrid = np.linspace(lower_lim, upper_lim, ngrid)\n",
    "\n",
    "X, Y = np.meshgrid(xgrid, ygrid) # Now we have (ngrid x ngrid) matrix\n",
    "\n",
    "# ravel() function flattens (ngrid x ngrid) matrix -> (1 x ngrid**2) array\n",
    "xy = np.vstack([X.ravel(), Y.ravel()]).T\n",
    "\n",
    "# Print the shapes!\n",
    "print('Shape of X and Y: ' + str(X.shape))\n",
    "print('Shape of xy: ' + str(xy.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define a wrapper function that runs KDE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define a wrapper function that runs KDE and returns the evaluated density $\\hat{p}(x)=  \\frac{1}{N} \\sum_{i=1}^{N} K\\left(x - x_i; h \\right)$ in `(ngrid X ngrid)` matrix form. <br/>\n",
    "It is better to define a wrapper function since this will be used a lot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_kde(Xdata, bandwidth, metric, kernel):\n",
    "    \"\"\" Construct a KernelDensity object, fit with the data points we generated, \n",
    "        and then return the evaluated density for the (ngrid X ngrid) mesh grid \"\"\"\n",
    "    # Construct a kernel density object\n",
    "    kde = KernelDensity(bandwidth=bandwidth, metric=metric, kernel=kernel)\n",
    "    kde.fit(Xdata)\n",
    "    # kde.score_samples() returns values in log scale\n",
    "    # xy is the flattened mesh grid that we defined earlier (used as global var.)\n",
    "    log_p_hat = kde.score_samples(xy)\n",
    "    phat = np.exp(log_p_hat)\n",
    "    phat = phat.reshape((ngrid, ngrid))\n",
    "    return phat\n",
    "\n",
    "print('Function run_kde() defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Plot the estimated density\n",
    "\n",
    "Now run the function defined above, and plot the estimated density $\\hat{p}(x)$ as contour plot. \n",
    "We're going to use Euclidean distance (`metric='euclidean`) and Gaussian kernel (`kernel='gaussian'`), and change the size of the bandwidth to see the difference in the result. <br/>\n",
    "(You can also try with different metrics or kernels : more information at [Scikit-learn KernelDensity](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KernelDensity.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Manually Selected Bandwidth\n",
    "\n",
    "Try running with different size of bandwidths to find the best bandwidth. Modify the variable **`selected_bw`** in the below code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate the estimated density using kernel density estimation\n",
    "selected_bw = 80\n",
    "\n",
    "phat = run_kde(m_data1, bandwidth=selected_bw, metric='euclidean', kernel='gaussian') \n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(11,5), sharex=True, sharey=True) \n",
    "ax = axs[0]\n",
    "levels = np.linspace(phat.min(), phat.max(), 20)\n",
    "im = ax.contourf(X, Y, phat, levels=levels, cmap='Blues')\n",
    "ax.set_xlim(lower_lim, upper_lim)\n",
    "ax.set_ylim(lower_lim, upper_lim)\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_title('Density from KDE (BW=%.2f)' % selected_bw)\n",
    "\n",
    "ax = axs[1]\n",
    "ax.scatter(m_data1[:,0], m_data1[:,1], s=35, c='blue', linewidth=0, alpha=0.6)\n",
    "ax.set_xlabel('X')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First make it into a function\n",
    "def run_kde_and_plot(data, bw, color='blue', cmap='Blues'):\n",
    "    # Generate the estimated density using kernel density estimation\n",
    "    phat = run_kde(data, bandwidth=bw, metric='euclidean', kernel='gaussian') \n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(11,5), sharex=True, sharey=True) \n",
    "    ax = axs[0]\n",
    "    levels = np.linspace(phat.min(), phat.max(), 20)\n",
    "    im = ax.contourf(X, Y, phat, levels=levels, cmap=cmap)\n",
    "    ax.set_xlim(lower_lim, upper_lim)\n",
    "    ax.set_ylim(lower_lim, upper_lim)\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_title('Density from KDE (BW=%.2f)' % bw)\n",
    "\n",
    "    ax = axs[1]\n",
    "    ax.scatter(data[:,0], data[:,1], s=35, c=color, linewidth=0, alpha=0.6)\n",
    "    ax.set_xlabel('X')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the same thing for the other two data\n",
    "run_kde_and_plot(m_data2, bw=selected_bw, color='red', cmap='Reds')\n",
    "run_kde_and_plot(m_data5, bw=selected_bw, color='green', cmap='Greens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Automated Bandwidth Selection\n",
    "\n",
    "Cross-validation can be used to select the bandwidth automatically. Cross-validation is a model validation technique for assessing how the results will generalize to an independent data set. In K-fold cross-validation, randomized data are splitted into K sets, and K-1 sets are used for estimating the density (train set) and 1 set is used for evaluation (validation set). We do this for K times, and score is calculated for the validation set at each run. The overall cross-validation score is the mean of the M scores. (More info: [Wikipedia: Cross-validation](https://en.wikipedia.org/wiki/Cross-validation_%28statistics%29))\n",
    "\n",
    "Scikit-learn has a nice package called **`GridSearchCV`** that does all the job for us! It uses **`score()`** function in the object to calculate the score. **`KernelDensity`** class has a function **`score(valX)`** that returns the total log probability of the validation data **`valX`** under the model. **`GridSearchCV`** will calculate the cross-validation score for each bandwidth value, and then return the bandwidth that gave the highest score. <br/>(Package info: [sklearn.model_selection.GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html))\n",
    "\n",
    "We will use 10-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "min_bw = 30\n",
    "max_bw = 120\n",
    "grid = GridSearchCV(KernelDensity(metric='euclidean', kernel='gaussian'),\n",
    "                    {'bandwidth': np.linspace(min_bw, max_bw, 50)}, cv=10) # 10-fold cross-validation\n",
    "grid.fit(m_data1)\n",
    "print(grid.best_params_)\n",
    "bw_cv = grid.best_params_['bandwidth'] # Bandwidth value saved in 'bw_cv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "run_kde_and_plot(m_data1, bw=bw_cv, color='blue', cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grid.fit(m_data2)\n",
    "print(grid.best_params_)\n",
    "bw_cv = grid.best_params_['bandwidth'] # Bandwidth value saved in 'bw_cv'\n",
    "\n",
    "run_kde_and_plot(m_data2, bw=bw_cv, color='red', cmap='Reds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grid.fit(m_data5)\n",
    "print(grid.best_params_)\n",
    "bw_cv = grid.best_params_['bandwidth'] # Bandwidth value saved in 'bw_cv'\n",
    "\n",
    "run_kde_and_plot(m_data5, bw=bw_cv, color='green', cmap='Greens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Difference between KDEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bw = 80\n",
    "\n",
    "phat1 = run_kde(m_data1, bandwidth=bw, metric='euclidean', kernel='gaussian') \n",
    "phat2 = run_kde(m_data2, bandwidth=bw, metric='euclidean', kernel='gaussian') \n",
    "phat5 = run_kde(m_data5, bandwidth=bw, metric='euclidean', kernel='gaussian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print out the maximum value of each function\n",
    "# This is to set up the range in the contour plot (plot them in the same scale)\n",
    "\n",
    "print(np.max(phat1))\n",
    "print(np.max(phat2))\n",
    "print( np.max(phat5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "levels = np.linspace(0, 7.5e-6, 20)\n",
    "\n",
    "fig, axs = plt.subplots(1,3, figsize=(13,4), sharey=True)\n",
    "ax = axs[0]\n",
    "im = ax.contourf(X, Y, phat1, levels=levels, cmap='Greys')\n",
    "ax.set_xlim(lower_lim, upper_lim)\n",
    "ax.set_ylim(lower_lim, upper_lim)\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_title('$\\hat{p}_1$', fontsize=18)\n",
    "\n",
    "ax = axs[1]\n",
    "im = ax.contourf(X, Y, phat2, levels=levels, cmap='Greys')\n",
    "ax.set_xlim(lower_lim, upper_lim)\n",
    "ax.set_ylim(lower_lim, upper_lim)\n",
    "ax.set_xlabel('X')\n",
    "ax.set_title('$\\hat{p}_2$', fontsize=18)\n",
    "\n",
    "ax = axs[2]\n",
    "im = ax.contourf(X, Y, phat5, levels=levels, cmap='Greys')\n",
    "ax.set_xlim(lower_lim, upper_lim)\n",
    "ax.set_ylim(lower_lim, upper_lim)\n",
    "ax.set_xlabel('X')\n",
    "ax.set_title('$\\hat{p}_5$', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print out the minimum and maximum values of the functions\n",
    "# This is to set up the range in the contour plot\n",
    "# We can see the biggest difference between data2 and data5\n",
    "\n",
    "print(np.min(phat1-phat2), np.max(phat1-phat2))\n",
    "print(np.min(phat1-phat5), np.max(phat1-phat5))\n",
    "print(np.min(phat2-phat5), np.max(phat2-phat5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalize the probability values to percentage\n",
    "max_val = np.max(phat2-phat5)\n",
    "\n",
    "phat12 = (phat1-phat2) / max_val * 100.0\n",
    "phat15 = (phat1-phat5) / max_val * 100.0\n",
    "phat25 = (phat2-phat5) / max_val * 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "levels = np.linspace(-100, 100, 20)\n",
    "\n",
    "fig, axs = plt.subplots(1,4, figsize=(13,4), gridspec_kw = {'width_ratios':[5,5,5,1]})\n",
    "ax = axs[0]\n",
    "im = ax.contourf(X, Y, phat12, levels=levels, cmap='RdBu')\n",
    "ax.set_xlim(lower_lim, upper_lim)\n",
    "ax.set_ylim(lower_lim, upper_lim)\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_title('$\\hat{p_1} - \\hat{p_2}$', fontsize=18)\n",
    "\n",
    "ax = axs[1]\n",
    "im = ax.contourf(X, Y, phat15, levels=levels, cmap='RdBu')\n",
    "ax.set_xlim(lower_lim, upper_lim)\n",
    "ax.set_ylim(lower_lim, upper_lim)\n",
    "ax.set_xlabel('X')\n",
    "ax.set_title('$\\hat{p_1} - \\hat{p_5}$', fontsize=18)\n",
    "\n",
    "ax = axs[2]\n",
    "im = ax.contourf(X, Y, phat25, levels=levels, cmap='RdBu')\n",
    "ax.set_xlim(lower_lim, upper_lim)\n",
    "ax.set_ylim(lower_lim, upper_lim)\n",
    "ax.set_xlabel('X')\n",
    "ax.set_title('$\\hat{p_2} - \\hat{p_5}$', fontsize=18)\n",
    "\n",
    "fig.colorbar(im, cax=axs[3])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
